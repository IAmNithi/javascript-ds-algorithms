## Big O Definition 

We say that an algorithm is **O(f(n))** if the number of simple operations that the computer has to do eventually less that the constant time **f(n)**, as **n** increases.

* **f(n)** could be linear **(f(n) = n)**
* **f(n)** could be quadratic **(f(n) = n^2)**
* **f(n)** could be constant **(f(n) = 1)**
* **f(n)** could be something entirely different!

 